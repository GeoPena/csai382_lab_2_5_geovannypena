{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae26eb7-8e54-43ba-8377-6d18aa9257af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Assignment 3.3: Automated ETL Pipeline\n",
    "\n",
    "####Geovanny Peña Rueda\n",
    "**Purpose:**  \n",
    "Automate the ETL pipeline to clean and label STEDI Step Test sensor data. This notebook:\n",
    "- Loads raw device messages and step tests\n",
    "- Converts and cleans columns\n",
    "- Labels sensor readings as `step` or `no_step`\n",
    "- Saves the curated dataset to the Silver layer\n",
    "- Performs verification queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06533e6-6bc9-42cb-b27d-1ebf5a4e3caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1 – Load Raw Bronze Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd1b257-02d7-4baf-b602-ef7b9dbab8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_device = spark.table(\"workspace.bronze.device_messages_raw\")\n",
    "df_steps = spark.table(\"workspace.bronze.rapid_step_tests_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3d30a1-8f48-453f-949a-aa2a0079f8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2 – Prepare Each Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f7c9cc-b70b-4b78-8cf0-5efed5cd0c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, lit, when\n",
    "\n",
    "# Convert 'distance' from string to integer (distance_cm)\n",
    "df_device = df_device.withColumn(\n",
    "    \"distance_cm\",\n",
    "    regexp_extract(col(\"distance\"), r\"(\\d+)\", 1).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Add source labels for traceability\n",
    "df_device = df_device.withColumn(\"source\", lit(\"device\"))\n",
    "df_steps = df_steps.withColumn(\"source\", lit(\"step\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3dcd89f-2992-48e0-b162-f61fe7af2739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3 – Label Each Sensor Reading (step / no_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2ead44-3bb7-4e3c-9e5e-31603f12332f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the step test window (start and stop times)\n",
    "df_steps_window = df_steps.select(\"device_id\", \"start_time\", \"stop_time\")\n",
    "\n",
    "# Label each sensor reading as 'step' or 'no_step'\n",
    "df_labeled = (\n",
    "    df_device.alias(\"d\")\n",
    "    .join(\n",
    "        df_steps_window.alias(\"s\"),\n",
    "        (col(\"d.device_id\") == col(\"s.device_id\")) &\n",
    "        (col(\"d.timestamp\").between(col(\"s.start_time\"), col(\"s.stop_time\"))),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"step_label\",\n",
    "        when(col(\"s.start_time\").isNotNull(), \"step\").otherwise(\"no_step\")\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046c1cc2-06cc-42ea-becf-71e7fac382b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4 – Select Final Curated Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175959c3-3b3c-484c-9a28-3c7183a4045a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_final = df_labeled.select(\n",
    "    col(\"d.timestamp\").alias(\"timestamp\"),\n",
    "    col(\"d.sensor_type\").alias(\"sensor_type\"),\n",
    "    col(\"distance_cm\"),\n",
    "    col(\"d.device_id\").alias(\"device_id\"),\n",
    "    col(\"step_label\"),\n",
    "    col(\"source\")\n",
    ")\n",
    "\n",
    "display(df_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc79bd4e-8bdf-4a78-8a6a-7a2c59f5f998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5 – Save Curated Silver Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f48f3af-b955-45f7-a628-64aa7779b1e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE workspace.silver\")\n",
    "\n",
    "# Create a temporary view so SQL can see it\n",
    "df_final.createOrReplaceTempView(\"df_final_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5660de97-afd9-474d-8cdc-53b606d2412d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This is the required final SQL cell for the ETL Job\n",
    "\n",
    "CREATE OR REPLACE TABLE labeled_step_test AS\n",
    "SELECT * FROM df_final_temp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9191278f-9d95-498e-ad29-491faa9287d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6 – Verification Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24da1cb1-8df9-4d8d-a154-e6a721a69071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Steps vs No-Steps\n",
    "SELECT step_label, COUNT(*) AS row_count\n",
    "FROM labeled_step_test\n",
    "GROUP BY step_label;\n",
    "\n",
    "-- 2. Invalid or missing step labels\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE step_label NOT IN ('step', 'no_step')\n",
    "OR step_label IS NULL\n",
    "LIMIT 50;\n",
    "\n",
    "-- 3. Source label counts\n",
    "SELECT source, COUNT(*) AS row_count\n",
    "FROM labeled_step_test\n",
    "GROUP BY source;\n",
    "\n",
    "\n",
    "-- 4. Invalid or missing source labels\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE source NOT IN ('device','step')\n",
    "OR source IS NULL\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b80ce0c-5494-4802-9e9b-393c008a21c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Part 7 – Ethics Reflection\n",
    "\n",
    "When automating health-related data pipelines, engineers must consider ethical responsibilities including:\n",
    "\n",
    "- **Privacy and Security:** Ensure personal and health data is protected.\n",
    "- **Data Accuracy and Validation:** Avoid labeling errors that could mislead analyses.\n",
    "- **Avoiding Bias:** Ensure models and labels do not unfairly target certain groups.\n",
    "- **Not Making Medical Claims:** Clearly communicate that the data is for analysis, not diagnosis.\n",
    "- **Protecting People, Not Just Data:** Ensure decisions from automated pipelines do not harm users.\n",
    "\n",
    "Automation must be done carefully to build trustworthy and ethical health data systems.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1175610910038545,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_4_stedi_etl_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
